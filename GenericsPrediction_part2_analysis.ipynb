{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi']\n",
    "\n",
    "import csv\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, timedelta as td, datetime\n",
    "\n",
    "from sklearn import base\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "def printmetrics(y_predict,y_true):\n",
    "    print \"Abs Mean Error: \", metrics.mean_absolute_error(y_true, y_predict)\n",
    "    print \"R2 Score: \", metrics.r2_score(y_true, y_predict)\n",
    "    print \"RMSE: \", np.sqrt(metrics.mean_squared_error(y_true, y_predict))\n",
    "\n",
    "def compute_error(clf, X, y):\n",
    "    return - cross_validation.cross_val_score(clf, X, y, cv=cv, scoring='mean_squared_error').mean()\n",
    "\n",
    "def side_by_side (*objs, **kwds):\n",
    "    from pandas.core.common import adjoin\n",
    "    space = kwds.get('space',4)\n",
    "    reprs = [repr(obj).split('\\n') for obj in objs]\n",
    "    print adjoin(space,*reprs)\n",
    "\n",
    "def unix_time(dt):\n",
    "    epoch = datetime.utcfromtimestamp(0)\n",
    "    delta = dt - epoch\n",
    "    \n",
    "    if isinstance(delta,pd.Series):\n",
    "        return delta.astype('timedelta64[s]')\n",
    "    else:\n",
    "        return delta.total_seconds()\n",
    "\n",
    "def unix_time_millis(dt):\n",
    "    return unix_time(dt)*1000\n",
    "\n",
    "\n",
    "# input is a pandas series\n",
    "def year_frac(pd_series):\n",
    "    return pd_series.apply(lambda x: x.year + float((x-datetime(x.year,1,1) ).days) / \\\n",
    "                           ( datetime(x.year+1,1,1)-datetime(x.year,1,1) ).days )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal: Predicting brand-to-generic conversion time\n",
    "* Import table from SQL\n",
    "* Combine the EPC and dosage into the same column\n",
    "* Create pipeline for combining the word features and the date feature.\n",
    "* Use RidgeCV for linear regression\n",
    "  * The CV avoids overfitting, especially on some less common pharmaceutical classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WordXFormer(base.BaseEstimator, base.TransformerMixin):\n",
    "        \n",
    "    def __init__(self ):\n",
    "        # init count vectorizer. This is the fitting \n",
    "        self.cv = CountVectorizer()\n",
    "#         self.cv = CountVectorizer(vocabulary=dict(zip(vocab,xrange(len(vocab)))))\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \n",
    "        # This sets the sparse matrix for all words to be counted.\n",
    "        return self.cv.fit_transform( X['epc_dosageform'] ).toarray()\n",
    "\n",
    "class DateXFormer(base.BaseEstimator, base.TransformerMixin):\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \n",
    "        # Actually return year and month separately.\n",
    "        # Possible to try with a \"julian\" date as well.\n",
    "        return list(zip(list(X['bxdate'].apply(lambda x: x.year)),list(X['bxdate'].apply(lambda x: x.month))))\n",
    "\n",
    "    \n",
    "class LinearEstimator(base.BaseEstimator, base.RegressorMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # Use a cross validated Ridge to avoid overfitting on particular words.\n",
    "        self.lr = RidgeCV()\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        self.lr.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "               \n",
    "        return self.lr.predict(X)\n",
    "    \n",
    "    def score(self, X,y):\n",
    "        \n",
    "        return self.lr.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CSVs:\n",
    "# firstbxgxmatch_fdalist - Grouped by the nonproprietaryname + dosage form as described by the Orange Book. bxdate - gxdate > 180.\n",
    "\n",
    "df = pd.read_csv('firstbxgxmatch_fdalist.csv', sep = ',')\n",
    "df['bxdatestr'] = df['bxdate']\n",
    "df['gxdatestr'] = df['gxdate']\n",
    "# Change to datetime format\n",
    "df['bxdate'] = pd.to_datetime(df['bxdate'])\n",
    "df['gxdate'] = pd.to_datetime(df['gxdate'])\n",
    "\n",
    "\n",
    "df_epc = df[df['epc'].notnull() & (df['bxdate'] > '1995-01-01')]\n",
    "df_epc['epc_dosageform'] = df_epc['epc'].map(str) + df_epc['dosageform'].map(str)\n",
    "        \n",
    "# Estimation of brand name date and generic name date for plot\n",
    "bxdate = df['bxdate'].apply(lambda x: x.year) + \\\n",
    "        df['bxdate'].apply(lambda x: x.month)/12 + \\\n",
    "        df['bxdate'].apply(lambda x: x.year)/365\n",
    "gxdate = df['gxdate'].apply(lambda x: x.year) + \\\n",
    "        df['gxdate'].apply(lambda x: x.month)/12 + \\\n",
    "        df['gxdate'].apply(lambda x: x.year)/365\n",
    "\n",
    "        \n",
    "plt.figure()\n",
    "plt.title('Brand vs Generic Release Dates')\n",
    "plt.xlabel('Brand Release Year')\n",
    "plt.plot(bxdate.values,gxdate.values,'b.')\n",
    "plt.ylabel('Generic Release Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model creation and fitting\n",
    "\n",
    "df_data = df_epc[['epc_dosageform','bxdate']]\n",
    "y_true = (df_epc['bxgxtime'].apply(lambda x: float(x))).values\n",
    "                   \n",
    "\n",
    "# Feature union\n",
    "feat_union = FeatureUnion([\n",
    "    ('wordx', WordXFormer()),   # EPC and Dose Count Vectorizer\n",
    "    ('datex', DateXFormer()),   # Date of Release\n",
    "  ])\n",
    "# Actual pipeline\n",
    "bxtogx_pipeline = Pipeline([('features',feat_union), ('lr',LinearEstimator())])\n",
    "\n",
    "# Fit model\n",
    "bxtogx_pipeline.fit(df_data,y_true)\n",
    "\n",
    "# Get prediction\n",
    "y_predict = bxtogx_pipeline.predict(df_data)\n",
    "\n",
    "\n",
    "print bxtogx_pipeline.score(df_data,y_true )\n",
    "\n",
    "printmetrics(y_predict/365,y_true/365)\n",
    "\n",
    "plt.plot(y_predict, y_true,'b.')\n",
    "plt.plot(xrange(1,6000), xrange(1,6000),'k-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creation of scatterplot for NVD3\n",
    "\n",
    "df_epc['predict'] = (y_predict)/365\n",
    "df_epc['true'] = (y_true)/365\n",
    "\n",
    "df_epc_after1995 = df_epc[( df_epc['bxdate'].apply(lambda x:x.year) > 1995 ) & ( df_epc['gxdate'].apply(lambda x:x.year) > 1995 )]\n",
    "df_epc_after1995 = df_epc[df_epc['predict'] > 365/180]\n",
    "\n",
    "json_obj = []\n",
    "df_epc_after1995 = df_epc_after1995.reset_index()\n",
    "print len(df_epc_after1995)\n",
    "\n",
    "for i in np.arange(len(df_epc_after1995)):\n",
    "        \n",
    "    json_obj.append( {'bxname': df_epc_after1995['bxname'][i] ,\n",
    "             'gxname': df_epc_after1995['gxname'][i] , \n",
    "             'epc': df_epc_after1995['epc'][i] , \\\n",
    "             'predict' : df_epc_after1995['predict'][i] , \\\n",
    "             'true' : df_epc_after1995['true'].loc[i] , \\\n",
    "             'bxdatestr' : df_epc_after1995['bxdatestr'][i] } )\n",
    "    \n",
    "    \n",
    "with open('epcdosemodel.json', 'w') as outfile:\n",
    "    json.dump(json_obj, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourier Transform Frequency Analysis\n",
    "* Create new dataframe where index is a range of dates\n",
    "* Join SQL csv to dataframe, and perform FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Supplier trends. How likely are they to release stuff, and when?\n",
    "# Look at the top suppliers first.\n",
    "df_majorsuppliers = pd.read_csv('major_supplier_release_dates.csv', sep = ',')\n",
    "df_majorsuppliers['startmktdate'] = pd.to_datetime(df_majorsuppliers['startmktdate'])\n",
    "\n",
    "# Bar graph by startmktdate MONTH\n",
    "years = [2009,2010,2011,2012,2013,2014, 2015]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "avg_dayseries =  df_majorsuppliers.groupby(df_majorsuppliers['startmktdate'] \\\n",
    "                                .apply(lambda x: ('0'+ str(x.month))[-2:] +  ('0'+ str(x.day))[-2:]))['prodid'].count()\n",
    "avg_dayseries.plot()\n",
    "plt.title('Time Series of Average Number of Gx Launches \\n (Averaged between 2010-2014)')\n",
    "plt.xlabel('Month-Day')\n",
    "\n",
    "for year in years:\n",
    "    df_majorsuppliers_year = df_majorsuppliers[df_majorsuppliers['startmktdate'].apply(lambda x: x.year == year)]\n",
    "    oneyear_dayseries =  df_majorsuppliers_year.groupby(df_majorsuppliers['startmktdate'] \\\n",
    "                                    .apply(lambda x: ('0'+ str(x.month))[-2:] +  ('0'+ str(x.day))[-2:]))['prodid'].count()\n",
    "    avg_dayseries = pd.concat([avg_dayseries, oneyear_dayseries], axis=1)\n",
    "\n",
    "avg_dayseries.columns = ['avg','2009','2010','2011','2012','2013','2014','2015']\n",
    "avg_dayseries = avg_dayseries.fillna(0)\n",
    "\n",
    "# Split becasue this groupby takes a while\n",
    "s_releases = df_majorsuppliers.sort(['startmktdate']).groupby(['startmktdate'])['startmktdate'].count()\n",
    "# General statistics:\n",
    "s_releases_after_2k9 = s_releases[s_releases.index.year >= 2009]\n",
    "print \"MEAN: \", np.mean(s_releases_after_2k9)\n",
    "print \"25%: \", np.percentile(s_releases_after_2k9,25)\n",
    "print \"50%: \", np.percentile(s_releases_after_2k9,50)\n",
    "print \"75%: \", np.percentile(s_releases_after_2k9,75)\n",
    "print \"95%: \", np.percentile(s_releases_after_2k9,95)\n",
    "print \"97.5%: \", np.percentile(s_releases_after_2k9,97.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting the fourier transform of all the releases\n",
    "\n",
    "start = date(2009, 1, 1)\n",
    "finish = date(2015, 6, 30)\n",
    "rng = pd.date_range(start,finish)\n",
    "df_majorsuppliers_ts = pd.DataFrame(index = rng)\n",
    "\n",
    "# Count the number of releases every day\n",
    "df_eachday =  df_majorsuppliers[df_majorsuppliers['startmktdate'] >= datetime(2009,1,1)].groupby(['startmktdate'])['startmktdate'].count()\n",
    "df_majorsuppliers_ts = df_majorsuppliers_ts.join(df_eachday).fillna(0)\n",
    "\n",
    "fs = 365 # Sampling rate in samples per year\n",
    "fft = np.fft.fft(df_majorsuppliers_ts.values,axis=0)\n",
    "xVals = fs * np.arange(float(len(fft)))/ len(fft) # Get frequency values for each of the fft values\n",
    "\n",
    "plt.plot(xVals, np.abs(fft))\n",
    "plt.ylim([0, 6E3]) # Zoom in on y axis\n",
    "plt.xlim([0, max(xVals)/2]) # Zoom in on x axis\n",
    "#Note: largest meaningful x axis value is max(xVals)/2\n",
    "plt.title('Frequency Analysis of Gx Launches Over 2010-2014')\n",
    "plt.xlabel('Frequency Of Releases')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.show()\n",
    "\n",
    "# Local maxima:\n",
    "print np.nonzero(np.abs(fft) > 3000)\n",
    "print xVals[339], xVals[2033]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing PCA on supplier monthly releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_monthly_releases_11to14 = pd.read_csv('monthly_releases_2011to2014.csv', sep = ',')\n",
    "yrs = df_monthly_releases_11to14['yr'].unique()\n",
    "mos = df_monthly_releases_11to14['mo'].unique()\n",
    "supplier = df_monthly_releases_11to14['supplier'].unique()\n",
    "\n",
    "df_monthly_index = pd.DataFrame(index=sorted(mos))\n",
    "df_yearly_release = pd.DataFrame(index=sorted(mos))\n",
    "\n",
    "# Group into dataframe where columns are YEAR + SUPPLIER and rows are MONTH (JAN-DEC)\n",
    "for supply in supplier:\n",
    "    for yr in yrs:\n",
    "        colname = str(yr) + ' ' + str(supply)\n",
    "        \n",
    "        df_tmp = df_monthly_releases_11to14[ (df_monthly_releases_11to14['yr'] == yr) & \\\n",
    "                                   (df_monthly_releases_11to14.supplier.apply(lambda x: x is supply)) ]\n",
    "\n",
    "        \n",
    "        \n",
    "        if df_tmp['count'].sum() > 10:\n",
    "            df_tmp.index = df_tmp['mo']\n",
    "            df_yearly_release[colname] = pd.concat([df_monthly_index,df_tmp['count']],axis=1)        \n",
    "\n",
    "df_yearly_release = df_yearly_release.fillna(0) \n",
    "df_yearly_release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preparing matrices for PCA by making them zero-mean\n",
    "\n",
    "mat = df_yearly_release.as_matrix().transpose()\n",
    "shp = mat.shape\n",
    "print (shp)\n",
    "print(mat.mean(axis=1).shape)\n",
    "# Have a 0 mean matrix\n",
    "mat_0u = mat - np.tile(mat.mean(axis=1), (shp[1],1)).transpose()\n",
    "\n",
    "# Also have a normalized matrix. This KILLS most of your variance, which makes a lot of sense. It should haha.\n",
    "mat_norm = np.multiply(mat,np.tile( (np.divide(1,mat.sum(axis=1))), (shp[1],1) ).transpose())\n",
    "mat_norm_0u = mat_norm - np.tile(mat_norm.mean(axis=1), (shp[1],1)).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform PCA, and get 12 components out. \n",
    "pca = PCA(n_components=12)\n",
    "X_r = pca.fit(mat_0u).transform(mat_0u)\n",
    "\n",
    "\n",
    "print('explained variance ratio (first 12 components): %s'\n",
    "      % str(pca.explained_variance_ratio_))\n",
    "print\n",
    "\n",
    "# Print first four components. \n",
    "print(np.round( pca.components_[:4]*1000)/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebuild all the components but using the eigenvectors/components as determined by the PCA by solving the formula:\n",
    "\n",
    "\n",
    "\n",
    "$$ COL = \\left[ \\begin{array}{ccc} \n",
    "PCA_1 \\\\\n",
    "\\vdots\\\\\n",
    "\\vdots\\\\\n",
    "\\end{array} \\right]  *  \n",
    "w_1 + \n",
    "\\left[ \\begin{array}{ccc} \n",
    "PCA_2 \\\\\n",
    "\\vdots\\\\\n",
    "\\vdots\\\\\n",
    "\\end{array} \\right]  *  \n",
    "w_2 + \n",
    "\\left[ \\begin{array}{ccc} \n",
    "PCA_3 \\\\\n",
    "\\vdots\\\\\n",
    "\\vdots\\\\\n",
    "\\end{array} \\right]  *  \n",
    "w_3 + ...$$\n",
    "\n",
    "### where COL is the supplier-year 12-column vector, PCA$_1$,$_2$,$_3$ are the principal components, and w$_1$,$_2$,$_3$ are the weights of the eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "components=[]\n",
    "suppliers = df_yearly_release.columns\n",
    "\n",
    "for num,row in enumerate(X_r):\n",
    "    tmp = np.dot(np.linalg.inv(pca.components_),np.matrix(row).T)\n",
    "    tmp = np.asarray(tmp).reshape(-1).tolist()\n",
    "    tmp.append(suppliers[num])\n",
    "    components.append(tuple( tmp ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# First components, top 10 suppliers that match\n",
    "com = 0\n",
    "\n",
    "sorted_components = sorted(components, key=lambda x: x[com], reverse=True)\n",
    "print (np.round(pca.components_[com]*100)/100)\n",
    "top10 = []\n",
    "bot10 = []\n",
    "weight = []\n",
    "for j in xrange(15):\n",
    "    weight.append( ( sorted_components[j][com], sorted_components[-j-1][com] ) )\n",
    "    top10.append(sorted_components[j][-1])\n",
    "    bot10.append(sorted_components[-j -1][-1])\n",
    "\n",
    "print (top10)\n",
    "print (bot10)\n",
    "# Uncomment to see feature vectors that are the top10/bot10\n",
    "# print (df_yearly_release[top10].as_matrix().transpose())\n",
    "# print (df_yearly_release[bot10].as_matrix().transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes on PCA\n",
    "* Since all components are orthogonal, it's probably good at representing the model after the second component\n",
    "* It also \"averages\" clusters to find the \"major\" component\n",
    "* A K-Nearest-Neighbors clustering algorithm is much more appropriate/intuitive at \"grouping\" the suppliers together\n",
    "* Below is a rough sample on how a K-Means clustering algorithm could work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Preliminary test on K-Means\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "\n",
    "mat = df_yearly_release.as_matrix().transpose()\n",
    "suppliers = df_yearly_release.columns\n",
    "\n",
    "KMeanEstimator = KMeans(n_clusters = 30, n_init=20, max_iter =1000)\n",
    "KMeanEstimator.fit(mat)\n",
    "labels = KMeanEstimator.labels_\n",
    "print labels\n",
    "print(mat.shape)\n",
    "\n",
    "\n",
    "supplier_clusters = [[]] * 30\n",
    "\n",
    "for num,label in enumerate(labels):\n",
    "    \n",
    "    if not supplier_clusters[label]:\n",
    "        supplier_clusters[label] = [suppliers[num][5:]]\n",
    "    else:\n",
    "        supplier_clusters[label].append(suppliers[num][5:])\n",
    "        \n",
    "# Count the number of times the same supplier appears in a cluster. Since they are grouped by years\n",
    "# a supplier can appear at MOST 4 times in a cluster.\n",
    "occur_twice = []\n",
    "occur_three = []\n",
    "occur_four = []\n",
    "\n",
    "for supplier_cluster in supplier_clusters:\n",
    "    dict_test = Counter(supplier_cluster)\n",
    "    tmp = [x for num,x in enumerate(dict_test.keys()) if dict_test.values()[num] == 3]\n",
    "    if tmp:\n",
    "        tmp.append(np.round(KMeanEstimator.cluster_centers_[labels[num]]*1000/1000).tolist())\n",
    "        occur_three.append(tmp)\n",
    "\n",
    "print occur_three\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### (Failed) Attempt to model suppliers\n",
    "\n",
    "####Features\n",
    "+ totsuppliers: Number of suppliers that release the brand name drug before the first ANDA is released\n",
    "+ totreleases: Number of brand names (as counted by unique NDCs) released\n",
    "+ julian_firstmktdate: The date the brand name was released (in julian format)\n",
    "+ Time for bxdate/gxdate conversion\n",
    "\n",
    "####To Predict:\n",
    "+ Number of generic suppliers 1 year after the release\n",
    "\n",
    "####Notes:\n",
    "* Preliminary model. Did not do much tweaking\n",
    "* Possible to improve if there were sales data on each brand name drug\n",
    "* Other possible features to help improve model: Drug type, dosage form/route, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print df_supplier_model.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_supplier_model = pd.read_csv('supplier_model.csv', sep = ',')\n",
    "df_supplier_model['td_firstmktdate'] = pd.to_datetime(df_supplier_model['firstmktdate'])\n",
    "df_supplier_model['julian_firstmktdate'] = df_supplier_model.td_firstmktdate.apply(lambda x: x.to_julian_date())\n",
    "\n",
    "# Xfeatures: totsuppliers, totreleases, firstmktdate, bxgxdiff\n",
    "# YModel: nsuppliers\n",
    "lr = LinearRegression()\n",
    "df_Xfeatures = df_supplier_model[['totsuppliers','totreleases','julian_firstmktdate', 'bxgxdiff']]\n",
    "lr.fit(df_Xfeatures, df_supplier_model['nsuppliers'])\n",
    "y_true = df_supplier_model['nsuppliers']\n",
    "y_predict = lr.predict(df_Xfeatures)\n",
    "\n",
    "# Essentially there is relationship. \n",
    "printmetrics(y_true,y_predict)\n",
    "plt.plot(y_true,y_predict,'b.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
